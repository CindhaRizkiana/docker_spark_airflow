{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the path to your PostgreSQL JDBC driver JAR file\n",
    "postgresql_jdbc_jar = \"/opt/bitnami/spark/jars/postgresql-42.2.18.jar\"\n",
    "spark_host = \"spark://dataeng-spark-master:7077\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "sparkcontext = pyspark.SparkContext.getOrCreate(conf=(\n",
    "        pyspark\n",
    "        .SparkConf()\n",
    "        .setAppName('ReadPostgres')\n",
    "        .setMaster(spark_host)\n",
    "        .set(\"spark.jars\", \"/opt/bitnami/spark/jars/postgresql-42.2.18.jar\")\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set log level\n",
    "sparkcontext.setLogLevel(\"WARN\")\n",
    "\n",
    "spark = pyspark.sql.SparkSession(sparkcontext.getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dataeng-jupyter:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://dataeng-spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ReadPostgres</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f16343dba60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PostgreSQL connection properties\n",
    "postgres_url = \"jdbc:postgresql://localhost:5432/postgres_db\"\n",
    "properties = {\n",
    "    \"user\": \"cinda\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Define the table name\n",
    "table_name = \"retail\"\n",
    "\n",
    "# Read data from PostgreSQL\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", postgres_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", properties[\"user\"]) \\\n",
    "    .option(\"password\", properties[\"password\"]) \\\n",
    "    .option(\"driver\", properties[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "# Display the content of the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any null values\n",
    "df = df.na.drop()\n",
    "\n",
    "# Change Data Types of selected columns to integers\n",
    "df = df.withColumn(\"invoiceno\", col(\"invoiceno\").cast(\"integer\"))\n",
    "df = df.withColumn(\"stockcode\", col(\"stockcode\").cast(\"integer\"))\n",
    "\n",
    "# Display the updated schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Customer Retention Analysis\n",
    "# Extract the month from the 'invoicedate' column\n",
    "df = df.withColumn(\"month\", F.month(\"invoicedate\"))\n",
    "# Group by month and calculate the number of unique customers\n",
    "monthly_unique_customers = df.groupBy(\"month\").agg(F.countDistinct(\"customerid\").alias(\"unique_customers\"))\n",
    "# Create a window specification to order the results by month\n",
    "windowSpec = Window().orderBy(\"month\")\n",
    "# Calculate retention rate by comparing current and previous month's unique customers\n",
    "monthly_unique_customers = monthly_unique_customers.withColumn(\"prev_unique_customers\", F.lag(\"unique_customers\").over(windowSpec))\n",
    "monthly_unique_customers = monthly_unique_customers.withColumn(\"retention_rate\", (monthly_unique_customers[\"unique_customers\"] / monthly_unique_customers[\"prev_unique_customers\"]).cast(\"double\"))\n",
    "# Display the results of retention analysis\n",
    "monthly_unique_customers.show()\n",
    "\n",
    "# Customer Churn Analysis\n",
    "# Find the maximum invoice date\n",
    "max_invoice_date = df.agg(F.max(\"invoicedate\")).collect()[0][0]\n",
    "# Filter customers from the last month\n",
    "last_month_customers = df.filter(F.month(\"invoicedate\") == F.month(F.lit(max_invoice_date)))\n",
    "# Find churned customers by subtracting last month's customers from all customers\n",
    "churned_customers = df.select(\"customerid\").distinct().subtract(last_month_customers.select(\"customerid\").distinct())\n",
    "# Count total customers\n",
    "total_customers = df.select(\"customerid\").distinct().count()\n",
    "# Calculate churn rate\n",
    "churn_rate = (F.lit(churned_customers.count()) / F.lit(total_customers)).cast(\"double\")\n",
    "# Display the churn rate\n",
    "print(f\"Churn Rate: {churn_rate}\")\n",
    "\n",
    "# Rank Operation\n",
    "# Group by customer and calculate total spending per customer\n",
    "total_spending_per_customer = df.groupBy(\"customerid\").agg(F.sum(\"unitprice\").alias(\"total_spending\"))\n",
    "# Rank customers based on total spending\n",
    "ranked_customers = total_spending_per_customer.withColumn(\"rank\", F.rank().over(Window.orderBy(F.desc(\"total_spending\"))))\n",
    "# Display the ranked customers\n",
    "ranked_customers.show()\n",
    "\n",
    "# Save the ranked_customers DataFrame to a CSV file\n",
    "csv_output_path = \"/mnt/wsl/docker_spark_airflow/sql/results/ranked_customers.csv\"\n",
    "ranked_customers.write.mode(\"overwrite\").csv(csv_output_path)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
